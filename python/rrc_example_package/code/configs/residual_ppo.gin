import rrc_simulation.code.residual_ppo

train.algorithm = @ResidualPPO2
train.maxt = 100000000
train.seed = 0
train.eval = True
train.eval_period = 10000000
train.save_period = 1000000
train.maxseconds = None

pi/optim.Adam.lr = 0.00005
vf/optim.Adam.lr = 0.0001
optim.Adam.betas = (0.9, 0.999)
optim.Adam.eps = 1e-5

ResidualPPO2.env_fn = @make_pybullet_env
ResidualPPO2.policy_fn = @torque_and_position_policy_fn
ResidualPPO2.value_fn = @value_fn
ResidualPPO2.nenv = 32
ResidualPPO2.eval_num_episodes = 100
ResidualPPO2.record_num_episodes = 0
ResidualPPO2.batch_size = 512
ResidualPPO2.rollout_length = 128
ResidualPPO2.gamma = 0.99
ResidualPPO2.lambda_ = 0.95
ResidualPPO2.norm_advantages = True
ResidualPPO2.opt_pi = @pi/optim.Adam
ResidualPPO2.opt_vf = @vf/optim.Adam
ResidualPPO2.epochs_pi = 10
ResidualPPO2.epochs_vf = 10
ResidualPPO2.kl_target = 0.01
ResidualPPO2.alpha = 1.5
ResidualPPO2.policy_training_start = 1000000
ResidualPPO2.max_grad_norm = 5.
ResidualPPO2.gpu = True

Checkpointer.ckpt_period = 100000

make_pybullet_env.reward_fn = "task1_reward_reg_slippage"
make_pybullet_env.termination_fn = "position_close_to_goal"
make_pybullet_env.initializer = "task1_init"
make_pybullet_env.action_space = "torque_and_position"
make_pybullet_env.init_joint_conf = True
make_pybullet_env.residual = True
make_pybullet_env.kp_coef = None
make_pybullet_env.kd_coef = None
make_pybullet_env.frameskip = 3
make_pybullet_env.seed = 0
make_pybullet_env.norm_observations = True
make_pybullet_env.visualization = False

VecObsNormWrapper.steps = 10000
VecObsNormWrapper.mean = None
VecObsNormWrapper.std = None
VecObsNormWrapper.eps = 1e-2
VecObsNormWrapper.log = True
VecObsNormWrapper.log_prob = 0.01
