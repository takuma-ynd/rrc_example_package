#!/usr/bin/env python3
'''
This script uses evaluation episodes generated by `generate_evaluation_episodes.py`
that are stored in eval_episodes/ directory to evaluate PPO policy.
It can also evaluate the scripted policies by specifying --residual_env_with_zero_policy
'''
import os
import argparse

import yaml
import json
import numpy as np
from stable_baselines import PPO2
from training_env import make_training_env
import pybullet as p
from utils import set_seed


def main(args):
    if args.residual_env_with_zero_policy:
        eval_config = {
            'action_space': 'torque_and_position',
            'frameskip': 3,
            'residual': True
        }
    else:
        policy_path = os.path.join(
            args.exp_dir, "training_checkpoints/model_" + str(args.time_steps) + "_steps"
        )
        model = PPO2.load(policy_path)

        with open(os.path.join(args.exp_dir, 'config.yaml'), 'r') as f:
            eval_config = yaml.load(f)['env']

    eval_config.update({
        'reward_fn': f'task{args.difficulty}_competition_reward',
        'termination_fn': 'position_close_to_goal',
        'initializer': f'task{args.difficulty}_eval_init'
    })

    env = make_training_env(visualization=False, **eval_config)
    num_episodes = len(env.initializer.episodes)
    acc_rewards = []
    for i in range(num_episodes):
        is_done = False
        observation = env.reset()
        accumulated_reward = 0
        while not is_done:
            if args.residual_env_with_zero_policy:
                action = env.action_space.sample() * 0
            else:
                action = model.predict(observation, deterministic=True)[0]
            observation, reward, is_done, info = env.step(action)
            accumulated_reward += reward
        acc_rewards.append(accumulated_reward)

        print("Episode {}\tAccumulated reward: {}".format(i, accumulated_reward))

        # store the log for evaluation. NOTE: each log file can take up ~2MB. Better to comment this out until the last moment.
        # env.platform.store_action_log(os.path.join(args.exp_dir, 'episode{:05d}_level{}_actions.log'.format(i, args.difficulty)))

    acc_rewards = np.asarray(acc_rewards)
    acc_rewards_summary = {'mean': acc_rewards.mean(), 'median': np.median(acc_rewards),
                           'max': acc_rewards.max(), 'min': acc_rewards.min(), 'stddev': acc_rewards.std()}

    print("=== Accumulated Rewards Summary ===")
    for key in ['mean', 'median', 'max', 'min', 'stddev']:
        print("acc_rewards ({})\t{}".format(key, acc_rewards_summary[key]))

    # save evaluation results
    eval_result_file = os.path.join(args.exp_dir, 'evalulation-level{}.json'.format(args.difficulty))
    with open(eval_result_file, 'w') as f:
        eval_result = json.dumps(acc_rewards_summary, indent=4, separators=(',', ': '), sort_keys=True)
        f.write(eval_result)
    print('Evaluation results are saved at {}'.format(eval_result_file))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--exp_dir", default=None, help="experiment_dir. Ignored if --residual_env_with_zero_policy is set")
    parser.add_argument("--time_steps", default=-1, type=int, help="time steps. Ignored if --residual_env_with_zero_policy is set")
    parser.add_argument("--difficulty", required=True, type=int, help="difficulty")
    parser.add_argument("--residual_env_with_zero_policy", action="store_true", help="evaluate scripted policy")
    args = parser.parse_args()

    if args.residual_env_with_zero_policy:
        args.exp_dir = 'eval_scripted_policy'
        os.makedirs(args.exp_dir)
    else:
        assert args.exp_dir is not None and args.time_steps >= 0

    set_seed()
    main(args)
